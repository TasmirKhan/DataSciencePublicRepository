{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa8b6d2",
   "metadata": {},
   "source": [
    "Analysis of Algorithms (Background)\n",
    "\n",
    "comments\n",
    "What is meant by Analysis of Algorithm?\n",
    "Algorithm analysis is an important part of computational complexity theory, which provides theoretical estimation for the required resources of an algorithm to solve a specific computational problem. Analysis of algorithms is the determination of the amount of time and space resources required to execute it.\n",
    "\n",
    "Why Analysis of Algorithms is important?\n",
    "To predict the behavior of an algorithm without implementing it on a specific computer.\n",
    "It is much more convenient to have simple measures for the efficiency of an algorithm than to implement the algorithm and test the efficiency every time a certain parameter in the underlying computer system changes.\n",
    "It is impossible to predict the exact behaviour of an algorithm. There are too many influencing factors.\n",
    "The analysis is thus only an approximation; it is not perfect.\n",
    "More importantly, by analyzing different algorithms, we can compare them to determine the best one for our purpose.\n",
    "Types of Algorithm Analysis:\n",
    "Best case\n",
    "Worst case\n",
    "Average case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6ee5f",
   "metadata": {},
   "source": [
    "Asymptotic Analysis in JS\n",
    "\n",
    "comments\n",
    "When analyzing algorithms, Asymptotic Analysis is a key concept. It helps us understand the order of growth of an algorithm's time or space requirements as the input size increases. Let’s break it down in simple terms and see how it applies to JavaScript.\n",
    "\n",
    "What is Asymptotic Analysis?\n",
    "Asymptotic analysis is a way to evaluate the performance of an algorithm by focusing on its growth rate as the input size (n) becomes very large. It ignores:\n",
    "\n",
    "Machine-dependent constants (e.g., hardware speed).\n",
    "Programming language differences.\n",
    "Smaller terms become insignificant for large inputs.\n",
    "Instead, it focuses on the big picture: how the algorithm scales with input size.\n",
    "\n",
    "Why Use Asymptotic Analysis?\n",
    "Machine-Independent: It doesn’t depend on the hardware or programming language.\n",
    "No Implementation Needed: You can analyze an algorithm without writing code.\n",
    "Focus on Growth Rate: It helps compare algorithms based on how they perform as the input size grows.\n",
    "Example: Sum of Natural Numbers\n",
    "Let’s analyze three different ways to calculate the sum of the first n natural numbers using asymptotic analysis.\n",
    "\n",
    "Function 1: Mathematical Formula\n",
    "\n",
    "function fun1(n) {\n",
    "    return (n * (n + 1)) / 2;\n",
    "}\n",
    "Explanation: This uses a mathematical formula to calculate the sum in constant time.\n",
    "Time Taken: c1 (where c1 is a constant).\n",
    "Time Complexity: O(1) – Constant time. The runtime doesn’t depend on the input size.\n",
    "Function 2: Single Loop\n",
    "\n",
    "\n",
    "\n",
    "function fun2(n) {\n",
    "    let sum = 0;\n",
    "    for (let i = 1; i <= n; i++) {\n",
    "        sum += i;\n",
    "    }\n",
    "    return sum;\n",
    "}\n",
    "Explanation: This uses a single loop to iterate through numbers from 1 to n and adds them to the sum.\n",
    "Time Taken: c2 * n + c3 (where c2 and c3 are constants).\n",
    "Time Complexity: O(n) – Linear time. The runtime grows linearly with the input size.\n",
    "Function 3: Nested Loops\n",
    "\n",
    "function fun3(n) {\n",
    "    let sum = 0;\n",
    "    for (let i = 1; i <= n; i++) {\n",
    "        for (let j = 1; j <= i; j++) {\n",
    "            sum++;\n",
    "        }\n",
    "    }\n",
    "    return sum;\n",
    "}\n",
    "Explanation: This uses nested loops to calculate the sum. The inner loop runs multiple times for each iteration of the outer loop.\n",
    "Time Taken: c4 * n² + c5 * n + c6 (where c4, c5, and c6 are constants).\n",
    "Time Complexity: O(n²) – Quadratic time. The runtime grows quadratically with the input size.\n",
    "How to Perform Asymptotic Analysis?\n",
    "Identify the Dominant Term:\n",
    "For large inputs, the term with the highest growth rate dominates.\n",
    "Ignore constants and lower-order terms.\n",
    "Example: In c4 * n² + c5 * n + c6, the dominant term is n².\n",
    "Use Big-O Notation:\n",
    "Big-O notation describes the upper bound of an algorithm's growth rate.\n",
    "Example: If an algorithm takes 3n² + 2n + 1 time, its time complexity is O(n²).\n",
    "Key Points to Remember\n",
    "Focus on Growth Rate: Asymptotic analysis focuses on how the algorithm performs as the input size grows.\n",
    "Ignore Constants: Constants (like c1, c2, etc.) are ignored because they don’t affect the growth rate.\n",
    "Compare Algorithms: Use asymptotic analysis to compare algorithms and choose the most efficient one.\n",
    "Conclusion\n",
    "Asymptotic analysis helps us understand how an algorithm's performance scales with input size, ignoring constants and focusing on growth rates. It allows us to compare algorithms and choose the most efficient one. By mastering this, you can write optimized and scalable code.\n",
    "\n",
    "Discussions\n",
    "( Threads )\n",
    "\n",
    "Most Recent\n",
    "User\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Commenting as Abbas Khan\n",
    "\n",
    "Comment Anonymously\n",
    "Submit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334075dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcc68e7d",
   "metadata": {},
   "source": [
    "Asymptotic Notations\n",
    "\n",
    "comments\n",
    "Asymptotic notations are mathematical tools to represent the time complexity of algorithms for asymptotic analysis. The following 3 asymptotic notations are mostly used to represent the time complexity of algorithms:\n",
    "\n",
    "thetanotation\n",
    "\n",
    "1) Θ Notation: The theta notation bounds a function from above and below, so it defines exact asymptotic behavior. \n",
    "A simple way to get the Theta notation of an expression is to drop low-order terms and ignore leading constants. For example, consider the following expression. \n",
    "3n3 + 6n2 + 6000 = Θ(n3) \n",
    "Dropping lower order terms is always fine because there will always be a number(n) after which Θ(n3) has higher values than Θ(n2) irrespective of the constants involved. \n",
    "For a given function g(n), we denote Θ(g(n)) is following set of functions. \n",
    "\n",
    "Θ(g(n)) = {f(n): there exist positive constants c1, c2 and n0 such \n",
    "                 that 0 <= c1*g(n) <= f(n) <= c2*g(n) for all n >= n0}\n",
    "\n",
    "The above definition means, if f(n) is theta of g(n), then the value f(n) is always between c1*g(n) and c2*g(n) for large values of n (n >= n0). The definition of theta also requires that f(n) must be non-negative for values of n greater than n0. \n",
    "\n",
    "\n",
    "BigO\n",
    "\n",
    "2) Big O Notation: The Big O notation defines an upper bound of an algorithm, it bounds a function only from above. For example, consider the case of Insertion Sort. It takes linear time in best case and quadratic time in worst case. We can safely say that the time complexity of Insertion sort is O(n^2). Note that O(n^2) also covers linear time.\n",
    "If we use O notation to represent time complexity of Insertion sort, we have to use two statements for best and worst cases:\n",
    "1. The worst case time complexity of Insertion Sort is O(n^2).\n",
    "2. The best case time complexity of Insertion Sort is O(n).\n",
    "\n",
    "The Big O notation is useful when we only have upper bound on time complexity of an algorithm. Many times we easily find an upper bound by simply looking at the algorithm.\n",
    " \n",
    "\n",
    "O(g(n)) = { f(n): there exist positive constants c and \n",
    "                  n0 such that 0 <= f(n) <= c*g(n) for \n",
    "                  all n >= n0}\n",
    "\n",
    "\n",
    "BigOmega\n",
    "\n",
    "3) Ω Notation: Just as Big O notation provides an asymptotic upper bound on a function, Ω notation provides an asymptotic lower bound.\n",
    "\n",
    "Ω Notation can be useful when we have lower bound on time complexity of an algorithm. The Omega notation is the least used notation among all three.\n",
    "\n",
    "For a given function g(n), we denote by Ω(g(n)) the set of functions.\n",
    " \n",
    "\n",
    "Ω (g(n)) = {f(n): there exist positive constants c and\n",
    "                  n0 such that 0 <= c*g(n) <= f(n) for\n",
    "                  all n >= n0}.\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "Discussions\n",
    "( Threads )\n",
    "\n",
    "Most Recent\n",
    "Commenting as Abbas Khan\n",
    "\n",
    "Comment Anonymously\n",
    "Submit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3d0434",
   "metadata": {},
   "source": [
    "Big O Notation\n",
    "\n",
    "comments\n",
    "\n",
    " In this article, we will discuss the analysis of the algorithm using Big - O asymptotic notation in complete detail. \n",
    "\n",
    "\n",
    " Big-O Analysis of Algorithms\n",
    "\n",
    "We can express algorithmic complexity using the big-O notation. For a problem of size N: \n",
    "\n",
    "A constant-time function/method is \"order 1\" : O(1)\n",
    "A linear-time function/method is \"order N\" : O(N)\n",
    "A quadratic-time function/method is \"order N squared\" : O(N2) \n",
    "Definition: Let g and f be functions from the set of natural numbers to itself. The function f is said to be O(g) (read big-oh of g), if there is a constant c > 0 and a natural number n0 such that f (n) ≤ cg(n) for all n >= n0 . \n",
    "\n",
    "Note: O(g) is a set!\n",
    "\n",
    "Abuse of notation: f = O(g) does not mean f ∈ O(g).\n",
    "The Big-O Asymptotic Notation gives us the Upper Bound Idea, mathematically described below: \n",
    "\n",
    " \n",
    "\n",
    "f(n) = O(g(n)) if there exists a positive integer n0 and a positive constant c, such that f(n)≤c.g(n) ∀ n≥n0 \n",
    "\n",
    "The general step wise procedure for Big-O runtime analysis is as follows:  \n",
    "\n",
    "Figure out what the input is and what n represents.\n",
    "Express the maximum number of operations, the algorithm performs in terms of n.\n",
    "Eliminate all excluding the highest order terms.\n",
    "Remove all the constant factors.\n",
    "Some of the useful properties of Big-O notation analysis are as follow: \n",
    "  \n",
    "\n",
    "▪ Constant Multiplication: \n",
    "If f(n) = c.g(n), then O(f(n)) = O(g(n)) ; where c is a nonzero constant. \n",
    "▪ Polynomial Function: \n",
    "If f(n) = a0 + a1.n + a2.n2 + ---- + am.nm, then O(f(n)) = O(nm). \n",
    "▪ Summation Function: \n",
    "If f(n) = f1(n) + f2(n) + ---- + fm(n) and fi(n)≤fi+1(n) ∀ i=1, 2, ----, m, \n",
    "then O(f(n)) = O(max(f1(n), f2(n), ----, fm(n))). \n",
    "▪ Logarithmic Function: \n",
    "If f(n) = logan and g(n)=logbn, then O(f(n))=O(g(n)) \n",
    "; all log functions grow in the same manner in terms of Big-O.\n",
    "\n",
    " Basically, this asymptotic notation is used to measure and compare the worst-case scenarios of algorithms theoretically. For any algorithm, the Big-O analysis should be straightforward as long as we correctly identify the operations that are dependent on n, the input size. \n",
    "\n",
    "\n",
    "Runtime Analysis of Algorithms\n",
    "\n",
    "In general cases, we mainly used to measure and compare the worst-case theoretical running time complexities of algorithms for the performance analysis. \n",
    "The fastest possible running time for any algorithm is O(1), commonly referred to as Constant Running Time. In this case, the algorithm always takes the same amount of time to execute, regardless of the input size. This is the ideal runtime for an algorithm, but it's rarely achievable. \n",
    "In actual cases, the performance (Runtime) of an algorithm depends on n, that is the size of the input or the number of operations is required for each input item. \n",
    "The algorithms can be classified as follows from the best-to-worst performance (Running Time Complexity): \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "▪ A logarithmic algorithm - O(logn) \n",
    "Runtime grows logarithmically in proportion to n. \n",
    "▪ A linear algorithm - O(n) \n",
    "Runtime grows directly in proportion to n. \n",
    "▪ A superlinear algorithm - O(nlogn) \n",
    "Runtime grows in proportion to n. \n",
    "▪ A polynomial algorithm - O(nc) \n",
    "Runtime grows quicker than previous all based on n. \n",
    "▪ A exponential algorithm - O(cn) \n",
    "Runtime grows even faster than polynomial algorithm based on n. \n",
    "▪ A factorial algorithm - O(n!) \n",
    "Runtime grows the fastest and becomes quickly unusable for even \n",
    "small values of n.   \n",
    "\n",
    "Where, n is the input size and c is a positive constant. \n",
    " \n",
    "\n",
    "asymtotic-analysis\n",
    "\n",
    "\n",
    "\n",
    "Algorithmic Examples of Runtime Analysis: \n",
    "Some of the examples of all those types of algorithms (in worst-case scenarios) are mentioned below: \n",
    " \n",
    "\n",
    "▪ Logarithmic algorithm - O(logn) - Binary Search. \n",
    "▪ Linear algorithm - O(n) - Linear Search. \n",
    "▪ Superlinear algorithm - O(nlogn) - Heap Sort, Merge Sort. \n",
    "▪ Polynomial algorithm - O(n^c) - Strassen’s Matrix Multiplication, Bubble Sort, Selection Sort, Insertion Sort, Bucket Sort. \n",
    "▪ Exponential algorithm - O(c^n) - Tower of Hanoi. \n",
    "▪ Factorial algorithm - O(n!) - Determinant Expansion by Minors, Brute force Search algorithm for Traveling Salesman Problem. \n",
    "\n",
    " \n",
    "\n",
    "Mathematical Examples of Runtime Analysis: \n",
    "The performances (Runtimes) of different orders of algorithms separate rapidly as n (the input size) gets larger. Let's consider the mathematical example:  \n",
    "\n",
    "If n = 10,                  If n=20,\n",
    "    log(10) = 1;                log(20) = 2.996;\n",
    "    10 = 10;                    20 = 20;\n",
    "    10log(10)=10;               20log(20)=59.9;\n",
    "    102=100;                    202=400;\n",
    "    210=1024;                    220=1048576;\n",
    "    10!=3628800;                20!=2.432902e+1818;\n",
    " \n",
    "\n",
    "Memory Footprint Analysis of Algorithms \n",
    "\n",
    "For performance analysis of an algorithm, runtime measurement is not only relevant metric but also we need to consider the memory usage amount of the program. This is referred to as the Memory Footprint of the algorithm, shortly known as Space Complexity. \n",
    "Here also, we need to measure and compare the worst case theoretical space complexities of algorithms for the performance analysis. \n",
    "It basically depends on two major aspects described below: \n",
    "\n",
    "Firstly, the implementation of the program is responsible for memory usage. For example, we can assume that recursive implementation always reserves more memory than the corresponding iterative implementation of a particular problem.\n",
    "And the other one is n, the input size or the amount of storage required for each item. For example, a simple algorithm with a high amount of input size can consume more memory than a complex algorithm with less amount of input size. \n",
    "Algorithmic Examples of Memory Footprint Analysis: The algorithms with examples are classified from the best-to-worst performance (Space Complexity) based on the worst-case scenarios are mentioned below:   \n",
    "\n",
    "▪ Ideal algorithm - O(1) - Linear Search, Binary Search,\n",
    "    Bubble Sort, Selection Sort, Insertion Sort, Heap Sort, Shell Sort.\n",
    "▪ Logarithmic algorithm - O(log n) - Merge Sort.\n",
    "▪ Linear algorithm - O(n) - Quick Sort.\n",
    "▪ Sub-linear algorithm - O(n+k) - Radix Sort.\n",
    " \n",
    "\n",
    "Space-Time Tradeoff and Efficiency \n",
    "\n",
    "There is usually a trade-off between optimal memory use and runtime performance. \n",
    "In general for an algorithm, space efficiency and time efficiency reach at two opposite ends and each point in between them has a certain time and space efficiency. So, the more time efficiency you have, the less space efficiency you have and vice versa. \n",
    "For example, Mergesort algorithm is exceedingly fast but requires a lot of space to do the operations. On the other side, Bubble Sort is exceedingly slow but requires the minimum space. \n",
    "At the end of this topic, we can conclude that finding an algorithm that works in less running time and also having less requirement of memory space, can make a huge difference in how well an algorithm performs. \n",
    "\n",
    "Example of Big-oh noatation:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "// C++ program to findtime complexity for single for loop\n",
    "#include <bits/stdc++.h>\n",
    "using namespace std;\n",
    "// main Code\n",
    "int main()\n",
    "{\n",
    "    //declare variable\n",
    "    int a = 0, b = 0;\n",
    "    //declare size \n",
    "    int N = 5, M = 5;\n",
    "    // This loop runs for N time\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        a = a + 5;\n",
    "    }\n",
    "    // This loop runs for M time\n",
    "    for (int i = 0; i < M; i++) {\n",
    "        b = b + 10;\n",
    "    }\n",
    "    //print value of a and b\n",
    "    cout << a << ' ' << b;\n",
    "    return 0;\n",
    "}\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Output\n",
    "25 50\n",
    " \n",
    "\n",
    "Explanation : \n",
    "First Loop runs N Time whereas Second Loop runs M Time. The calculation takes O(1)space.\n",
    "Since both loops run independently (sequentially), So by adding them the time complexity will be O ( N + M ) = O( N + M). \n",
    "\n",
    "Time Complexity : O( N + M)\n",
    " \n",
    "\n",
    "Discussions\n",
    "( Threads )\n",
    "\n",
    "Most Recent\n",
    "Commenting as Abbas Khan\n",
    "\n",
    "Comment Anonymously\n",
    "Submit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2d93f",
   "metadata": {},
   "source": [
    "Omega Notation\n",
    "\n",
    "comments\n",
    "This article will discuss Big - Omega Notation represented by a Greek letter (Ω).\n",
    "\n",
    "Definition: Let g and f be the function from the set of natural numbers to itself. The function f is said to be Ω(g), if there is a constant c > 0 and a natural number n0 such that c*g(n) ≤ f(n) for all n ≥ n0\n",
    "\n",
    "Mathematical Representation:\n",
    "\n",
    " \n",
    "\n",
    "Ω(g) = {f(n): there exist positive constants c and n0 such that 0 ≤ c*g(n) ≤ f(n) for all n ≥ n0} \n",
    "Note: Ω (g) is a set\n",
    " \n",
    "\n",
    "Graphical Representation:\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Graphical Representation\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "In simple language, Big - Omega (Ω) notation specifies the asymptotic (at the extreme) lower bound for a function f(n).\n",
    " \n",
    "\n",
    "Follow the steps below to calculate Big - Omega (Ω) for any program:\n",
    "\n",
    "Break the program into smaller segments.\n",
    "Find the number of operations performed for each segment(in terms of the input size) assuming the given input is such that the program takes the least amount of time.\n",
    "Add up all the operations and simplify it, let's say it is f(n).\n",
    "Remove all the constants and choose the term having the least order or any other function which is always less than f(n) when n tends to infinity, let say it is g(n) then, Big - Omega (Ω) of f(n) is Ω(g(n)).\n",
    " \n",
    "\n",
    "Example: Consider an example to print all the possible pairs of an array. The idea is to run two nested loops to generate all the possible pairs of the given array. \n",
    "\n",
    "The pseudo-code is as follows:\n",
    "\n",
    "int print(int a[], int n)\n",
    "{\n",
    "    for (int i = 0; i < n; i++) \n",
    "    {\n",
    "        for (int j = 0; j < n; j++)\n",
    "        {\n",
    "            if(i != j)\n",
    "                cout << a[i] << \" \" \n",
    "                     << a[j] << \"\\n\";\n",
    "        }\n",
    "    }\n",
    "}\n",
    "Below is the implementation of the above approach:\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "// Java program for the above approach\n",
    "import java.lang.*;\n",
    "import java.util.*;\n",
    "​\n",
    "class GFG{\n",
    "​\n",
    "// Function to print all possible pairs\n",
    "static void print(int a[], int n)\n",
    "{\n",
    "    for(int i = 0; i < n; i++) \n",
    "    {\n",
    "        for(int j = 0; j < n; j++) \n",
    "        {\n",
    "            if (i != j)\n",
    "                System.out.println(a[i] + \" \" + a[j]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "​\n",
    "// Driver code\n",
    "public static void main(String[] args)\n",
    "{\n",
    "    \n",
    "    // Given array\n",
    "    int a[] = { 1, 2, 3 };\n",
    "​\n",
    "    // Store the size of the array\n",
    "    int n = a.length;\n",
    "​\n",
    "    // Function Call\n",
    "    print(a, n);\n",
    "}\n",
    "}\n",
    "​\n",
    "// This code is contributed by avijitmondal1998\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Output\n",
    "1 2\n",
    "1 3\n",
    "2 1\n",
    "2 3\n",
    "3 1\n",
    "3 2\n",
    " \n",
    "\n",
    "In this example, it is evident that the print statement gets executed n2 times therefore if the running time vs n graph is plotted a parabolic graph will be obtained, f(n2). Now linear functions g(n), logarithmic functions g(log n), constant functions g(1) all are less than a parabolic function when the input range tends to infinity therefore, the worst-case running time of this program can be Ω(log n), Ω(n), Ω(1), or any function g(n) which is less than n2 when n tends to infinity. See the below graphical representation:\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "When to use Big - Ω notation: Big - Ω notation is the least used notation for the analysis of algorithms because it can make a correct but imprecise statement over the performance of an algorithm. Suppose a person takes 100 minutes to complete a task, using Ω notation, it can be stated that the person takes more than 10 minutes to do the task, this statement is correct but not precise as it doesn't mention the upper bound of the time taken. Similarly, using Ω notation we can say that the worst-case running time for the binary search is Ω(1), which is true because we know that binary search would at least take constant time to execute.\n",
    "\n",
    "Discussions\n",
    "( Threads )\n",
    "\n",
    "Most Recent\n",
    "Commenting as Abbas Khan\n",
    "\n",
    "Comment Anonymously\n",
    "Submit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eae8e5",
   "metadata": {},
   "source": [
    "Theta Notation\n",
    "\n",
    "comments\n",
    "This article will discuss Big - Theta notations represented by a Greek letter (Θ).\n",
    "\n",
    "Definition: Let g and f be the function from the set of natural numbers to itself. The function f is said to be Θ(g), if there are constants c1, c2 > 0 and a natural number n0 such that c1* g(n) ≤ f(n) ≤ c2 * g(n) for all n ≥ n0 \n",
    "\n",
    "Mathematical Representation: \n",
    "\n",
    " \n",
    "\n",
    "Θ (g(n)) = {f(n): there exist positive constants c1, c2 and n0 such that 0 ≤ c1 * g(n) ≤ f(n) ≤ c2 * g(n) for all n ≥ n0}\n",
    "Note: Θ(g) is a set\n",
    "\n",
    "The above definition means, if f(n) is theta of g(n), then the value f(n) is always between c1 * g(n) and c2 * g(n) for large values of n (n ≥ n0). The definition of theta also requires that f(n) must be non-negative for values of n greater than n0. \n",
    "\n",
    "\n",
    "Graphical Representation:\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Graphical Representation\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "In simple language, Big - Theta(Θ) notation specifies asymptotic bounds (both upper and lower) for a function f(n) and provides the average time complexity of an algorithm. \n",
    "\n",
    " \n",
    "\n",
    "Follow the steps below to find the average time complexity of any program:\n",
    "\n",
    "Break the program into smaller segments.\n",
    "Find all types and number of inputs and calculate the number of operations they take to be executed. Make sure that the input cases are equally distributed.\n",
    "Find the sum of all the calculated values and divide the sum by the total number of inputs let say the function of n obtained is g(n) after removing all the constants, then in Θ notation its represented as Θ(g(n))\n",
    "Example 1:\n",
    "\n",
    "var doLinearSearch = function(array, targetValue) \n",
    "{ \n",
    "    for (var guess = 0; guess < array.length; guess++) \n",
    "    {\n",
    "        if (array[guess] === targetValue) \n",
    "        {\n",
    "            return guess;  // found it!   \n",
    "         }\n",
    "    }\n",
    "    return -1;  // didn't find it\n",
    "};\n",
    "Each time the for-loop iterates, it has to do several things-\n",
    "\n",
    "compare guess with array.length\n",
    "compare array[guess] with targetValue\n",
    "possibly return the value of guess\n",
    "increment guess.\n",
    "So the complexity will be theta n.\n",
    "\n",
    "Example 2: Consider an example to find whether a key exists in an array or not using linear search. The idea is to traverse the array and check every element if it is equal to the key or not.\n",
    "\n",
    "The pseudo-code is as follows:\n",
    "\n",
    "bool linearSearch(int a[], int n, int key)\n",
    "{\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (a[i] == key)\n",
    "            return true;\n",
    "    }\n",
    "\n",
    "    return false;\n",
    "}\n",
    "Below is the implementation of the above approach:\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "// Java program for the above approach\n",
    "import java.lang.*;\n",
    "import java.util.*;\n",
    "​\n",
    "class GFG{\n",
    "​\n",
    "// Function to find whether a key exists in an \n",
    "// array or not using linear search\n",
    "static boolean linearSearch(int a[], int n, \n",
    "                            int key)\n",
    "{\n",
    "    \n",
    "    // Traverse the given array, a[]\n",
    "    for(int i = 0; i < n; i++) \n",
    "    {\n",
    "        \n",
    "        // Check if a[i] is equal to key\n",
    "        if (a[i] == key)\n",
    "            return true;\n",
    "    }\n",
    "    return false;\n",
    "}\n",
    "​\n",
    "// Driver code\n",
    "public static void main(String[] args)\n",
    "{\n",
    "    \n",
    "    // Given Input\n",
    "    int arr[] = { 2, 3, 4, 10, 40 };\n",
    "    int x = 10;\n",
    "    int n = arr.length;\n",
    "​\n",
    "    // Function Call\n",
    "    if (linearSearch(arr, n, x))\n",
    "        System.out.println(\"Element is present in array\");\n",
    "    else\n",
    "        System.out.println(\"Element is not present in array\");\n",
    "}\n",
    "}\n",
    " \n",
    "\n",
    "\n",
    "Output\n",
    "Element is present in array\n",
    " \n",
    "\n",
    "In a linear search problem, let's assume that all the cases are uniformly distributed (including the case when the key is absent in the array). So, sum all the cases (when the key is present at position 1, 2, 3, ......, n and not present, and divide the sum by n + 1. \n",
    "\n",
    " \n",
    "\n",
    "Average case time complexity = \n",
    "\n",
    "⇒ \n",
    "\n",
    "⇒ \n",
    "\n",
    "⇒ \n",
    " (constants are removed)\n",
    "\n",
    " \n",
    "\n",
    "When to use Big - Θ notation: Big - Θ analyzes an algorithm with most precise accuracy since while calculating Big - Θ, a uniform distribution of different type and length of inputs are considered, it provides the average time complexity of an algorithm, which is most precise while analyzing, however in practice sometimes it becomes difficult to find the uniformly distributed set of inputs for an algorithm, in that case, Big-O notation is used which represent the asymptotic upper bound of a function f.\n",
    "\n",
    "Discussions\n",
    "( Threads )\n",
    "\n",
    "Most Recent\n",
    "Commenting as Abbas Khan\n",
    "\n",
    "Comment Anonymously\n",
    "Submit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b015d30e",
   "metadata": {},
   "source": [
    "Analysis of Common loops\n",
    "\n",
    "comments\n",
    "In this article, an analysis of iterative programs with simple examples is discussed. \n",
    "\n",
    "1) O(1): Time complexity of a function (or set of statements) is considered as O(1) if it doesn't contain loop, recursion, and call to any other non-constant time function. \n",
    " \n",
    "\n",
    "   // set of non-recursive and non-loop statements\n",
    "\n",
    "For example, swap() function has O(1) time complexity. \n",
    "A loop or recursion that runs a constant number of times is also considered as O(1). For example, the following loop is O(1). \n",
    " \n",
    "\n",
    "   // Here c is a constant   \n",
    "   for (int i = 1; i <= c; i++) {  \n",
    "        // some O(1) expressions\n",
    "   }\n",
    "\n",
    "2) O(n): Time Complexity of a loop is considered as O(n) if the loop variables are incremented/decremented by a constant amount. For recursive call in recursive function, the time complexity is considered as O(n). For example following functions have O(n) time complexity. \n",
    " \n",
    "\n",
    "   // Here c is a positive integer constant   \n",
    "   for (int i = 1; i <= n; i += c) {  \n",
    "        // some O(1) expressions\n",
    "   }\n",
    "\n",
    "   for (int i = n; i > 0; i -= c) {\n",
    "        // some O(1) expressions\n",
    "   }\n",
    " \n",
    "\n",
    "//Recursive function\n",
    "void recurse(n)\n",
    "{\n",
    "    if(n==0)\n",
    "        return;\n",
    "    else{\n",
    "        // some O(1) expressions\n",
    "    }\n",
    "    recurse(n-1);\n",
    "}\n",
    "\n",
    "3) O(nc): Time complexity of nested loops is equal to the number of times the innermost statement is executed. For example, the following sample loops have O(n2) time complexity \n",
    " \n",
    "\n",
    "  \n",
    "   for (int i = 1; i <=n; i += c) {\n",
    "       for (int j = 1; j <=n; j += c) {\n",
    "          // some O(1) expressions\n",
    "       }\n",
    "   }\n",
    "\n",
    "   for (int i = n; i > 0; i -= c) {\n",
    "       for (int j = i+1; j <=n; j += c) {\n",
    "          // some O(1) expressions\n",
    "   }\n",
    "\n",
    "For example, Selection sort and Insertion Sort have O(n2) time complexity. \n",
    "\n",
    "\n",
    "4) O(Logn) Time Complexity of a loop is considered as O(Logn) if the loop variables are divided/multiplied by a constant amount. \n",
    " \n",
    "\n",
    "   for (int i = 1; i <=n; i *= c) {\n",
    "       // some O(1) expressions\n",
    "   }\n",
    "   for (int i = n; i > 0; i /= c) {\n",
    "       // some O(1) expressions\n",
    "   }\n",
    "\n",
    "For example, Binary Search(refer iterative implementation) has O(Logn) time complexity. Let us see mathematically how it is O(Log n). The series that we get in the first loop is 1, c, c2, c3, ... ck. If we put k equals to Logcn, we get cLogcn which is n. \n",
    "\n",
    "\n",
    "5) O(LogLogn) Time Complexity of a loop is considered as O(LogLogn) if the loop variables are reduced/increased exponentially by a constant amount. \n",
    " \n",
    "\n",
    "   // Here c is a constant greater than 1   \n",
    "   for (int i = 2; i <=n; i = pow(i, c)) { \n",
    "       // some O(1) expressions\n",
    "   }\n",
    "   //Here fun is sqrt or cuberoot or any other constant root\n",
    "   for (int i = n; i > 1; i = fun(i)) { \n",
    "       // some O(1) expressions\n",
    "   }\n",
    "\n",
    "See this for mathematical details.\n",
    "\n",
    " \n",
    "How to combine the time complexities of consecutive loops? \n",
    "When there are consecutive loops, we calculate time complexity as a sum of time complexities of individual loops. \n",
    " \n",
    "\n",
    "   for (int i = 1; i <=m; i += c) {  \n",
    "        // some O(1) expressions\n",
    "   }\n",
    "   for (int i = 1; i <=n; i += c) {\n",
    "        // some O(1) expressions\n",
    "   }\n",
    "   Time complexity of above code is O(m) + O(n) which is O(m+n)\n",
    "   If m == n, the time complexity becomes O(2n) which is O(n).   \n",
    "\n",
    "How to calculate time complexity when there are many if, else statements inside loops? \n",
    "As discussed here, worst-case time complexity is the most useful among best, average and worst. Therefore we need to consider the worst case. We evaluate the situation when values in if-else conditions cause a maximum number of statements to be executed. \n",
    "For example, consider the linear search function where we consider the case when an element is present at the end or not present at all. \n",
    "When the code is too complex to consider all if-else cases, we can get an upper bound by ignoring if-else and other complex control statements. \n",
    "\n",
    "\n",
    "How to calculate the time complexity of recursive functions? \n",
    "The time complexity of a recursive function can be written as a mathematical recurrence relation. To calculate time complexity, we must know how to solve recurrences.\n",
    "\n",
    "The following is a cheat sheet of the time complexities of various algorithms.\n",
    "\n",
    "Algorithms Cheat Sheet\n",
    "\n",
    "Algorithm\tBest Case\tAverage Case\tWorst Case\n",
    "Selection Sort\tO(n^2)\tO(n^2)\tO(n^2)\n",
    "Bubble Sort\tO(n)\tO(n^2)\tO(n^2)\n",
    "Insertion Sort\tO(n)\tO(n^2)\tO(n^2)\n",
    "Tree Sort\tO(nlogn)\tO(nlogn)\tO(n^2)\n",
    "Radix Sort\tO(dn)\tO(dn)\tO(dn)\n",
    "Merge Sort\tO(nlogn)\tO(nlogn)\tO(nlogn)\n",
    "Heap Sort\tO(nlogn)\tO(nlogn)\tO(nlogn)\n",
    "Quick Sort\tO(nlogn)\tO(nlogn)\tO(n^2)\n",
    "Bucket Sort\tO(n+k)\tO(n+k)\tO(n^2)\n",
    "Counting Sort\tO(n+k)\tO(n+k)\tO(n+k)\n",
    " \n",
    "\n",
    "Discussions\n",
    "( Threads )\n",
    "\n",
    "Most Recent\n",
    "User\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Commenting as Abbas Khan\n",
    "\n",
    "Comment Anonymously\n",
    "Submit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54557b9",
   "metadata": {},
   "source": [
    "Analysis of Recursion\n",
    "\n",
    "comments\n",
    "Recursive algorithms are functions that call themselves to solve smaller instances of the same problem. Analyzing their time complexity involves understanding recurrence relations, which describe how the problem size reduces with each recursive call. Let’s break it down with examples.\n",
    "\n",
    "What is a Recursive Function?\n",
    "A recursive function is a function that calls itself during its execution. For example:\n",
    "\n",
    "\n",
    "function fun(n) {\n",
    "    if (n <= 0) return; \n",
    "    console.log(\"GFG\"); \n",
    "    fun(n / 2); // Recursive call\n",
    "    fun(n / 2); // Recursive call\n",
    "}\n",
    "Recurrence Relation\n",
    "To analyze recursive algorithms, we use recurrence relations, which express the time complexity in terms of smaller inputs.\n",
    "\n",
    "Example 1: Simple Recursion\n",
    "\n",
    "function fun(n) {\n",
    "    if (n <= 0) return; \n",
    "    console.log(\"GFG\"); \n",
    "    fun(n / 2); // Recursive call\n",
    "    fun(n / 2); // Recursive call\n",
    "}\n",
    "\n",
    "\n",
    "Recurrence Relation: \n",
    "T(n)=2T(n/2)+Θ(1)\n",
    "2T(n/2): Two recursive calls with half the input size.\n",
    "Θ(1): Constant work done in each call.\n",
    "Base Case: T(0)=Θ(1).\n",
    "Time Complexity: Θ(n).\n",
    "Example 2: Recursion with a Loop\n",
    "\n",
    "function fun(n) {\n",
    "    if (n == 0) return; // Base case\n",
    "    for (let i = 0; i < n; i++) { // Θ(n)\n",
    "        console.log(\"GFG\");\n",
    "    }\n",
    "    fun(n / 2); // T(n/2)\n",
    "    fun(n / 3); // T(n/3)\n",
    "}\n",
    "Recurrence Relation: \n",
    "T(n)=T(n/2)+T(n/3)+Θ(n)\n",
    "Base Case: T(0)=Θ(1)\n",
    "Time Complexity: Θ(n) \n",
    "Example 3: Linear Recursion\n",
    "\n",
    "function fun(n) {\n",
    "    if (n == 1) return; // Base case\n",
    "    console.log(\"GFG\"); // Θ(1)\n",
    "    fun(n - 1); // T(n-1)\n",
    "}\n",
    "Recurrence Relation: T(n)=T(n−1)+Θ(1)\n",
    "Base Case: T(1)=Θ(1)\n",
    "Time Complexity: Θ(n).\n",
    "Conclusion\n",
    "In this article, we studied how to analyze recursive algorithms using recurrence relations. We explored examples like simple recursion, recursion with loops, and linear recursion, and learned how to express their time complexity. By understanding recurrence relations and base cases, you can determine the efficiency of recursive functions and improve their performance.\n",
    "\n",
    "Discussions\n",
    "( Threads )\n",
    "\n",
    "Most Recent\n",
    "Commenting as Abbas Khan\n",
    "\n",
    "Comment Anonymously\n",
    "Submit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8b171",
   "metadata": {},
   "source": [
    "Recursion Tree Method for Solving Recurrences\n",
    "\n",
    "comments\n",
    "The Recursion Tree Method is a way of solving recurrence relations. In this method, a recurrence relation is converted into recursive trees. Each node represents the cost incurred at various levels of recursion. To find the total cost, costs of all levels are summed up.\n",
    "\n",
    "Steps to solve recurrence relation using recursion tree method: \n",
    "\n",
    "Draw a recursive tree for given recurrence relation\n",
    "Calculate the cost at each level and count the total no of levels in the recursion tree.\n",
    "Count the total number of nodes in the last level and calculate the cost of the last level\n",
    "Sum up the cost of all the levels in the recursive tree\n",
    "Let us see how to solve these recurrence relations with the help of some examples:\n",
    "\n",
    "\n",
    "Question 1: T(n) = 2T(n/2) + c\n",
    "\n",
    "Solution: \n",
    "\n",
    "Step 1: Draw a recursive tree \n",
    "Recursion Tree\n",
    "Recursion Tree\n",
    "Step 2: Calculate the work done or cost at each level and count total no of levels in recursion tree \n",
    "Recursive Tree with each level cost\n",
    "Recursive Tree with each level cost\n",
    "Count the total number of levels - \n",
    "\n",
    "Choose the longest path from root node to leaf node\n",
    "\n",
    " n/20 -→ n/21 -→ n/22 -→ ......... -→ n/2k\n",
    "\n",
    "Size of problem at last level = n/2k\n",
    "\n",
    " At last level size of problem becomes 1\n",
    "\n",
    " n/2k = 1\n",
    "\n",
    " 2k = n\n",
    "\n",
    "  k = log2(n)   \n",
    "\n",
    "Total no of levels  in recursive tree = k +1 = log2(n) + 1\n",
    "\n",
    "Step 3: Count total number of nodes in the last level and calculate cost of last level\n",
    " No. of nodes at level 0 = 20 = 1\n",
    "\n",
    "  No. of nodes at level 1 = 21 = 2\n",
    "\n",
    " ...............................................................\n",
    "\n",
    " No. of nodes at level log2(n) = 2log2(n) = nlog2(2) = n\n",
    "\n",
    " Cost of sub problems at level log2(n) (last level) = nxT(1) = n x c = nc  \n",
    "\n",
    "Step 4: Sum up the cost all the levels in recursive tree  \n",
    "   T(n) = c + 2c + 4c + ---- + (no. of levels-1) times + last level cost\n",
    "\n",
    " = c + 2c + 4c + ---- + log2(n) times + Θ(n)\n",
    "\n",
    " = c(1 + 2 + 4 + ---- + log2(n) times) + Θ(n)\n",
    "\n",
    " 1 + 2 + 4 + ----- + log2(n) times --> 20 + 21 + 22 + ----- + log2(n) times --> Geometric Progression(G.P.)\n",
    "\n",
    "= c(n) + Θ(n)\n",
    "\n",
    "Thus, T(n) = Θ(n)\n",
    "\n",
    "Question 2: T(n) = T(n/10) + T(9n/10) + n\n",
    "\n",
    "Solution: \n",
    "\n",
    "Step 1: Draw a recursive tree\n",
    "img1-300x129\n",
    "\n",
    "\n",
    "Step 2: Calculate the work done or cost at each level and count total no of levels in recursion tree\n",
    "Recursion Tree with each level cost\n",
    "Recursion Tree with each level cost\n",
    " Count the total number of levels -\n",
    "\n",
    " Choose the longest path from root node to leaf node\n",
    "\n",
    "(9/10)0n --> (9/10)1n --> (9/10)2n --> ......... --> (9/10)kn\n",
    "\n",
    " Size of problem at last level = (9/10)kn\n",
    "\n",
    " At last level size of problem becomes 1\n",
    "\n",
    "(9/10)kn = 1\n",
    "\n",
    "(9/10)k = 1/n\n",
    "\n",
    "   k = log10/9(n)\n",
    "\n",
    " Total no of levels in recursive tree = k +1 = log10/9(n) + 1\n",
    "\n",
    "Step 3: Count total number of nodes in the last level and calculate cost of last level\n",
    "No. of nodes at level 0 = 20 = 1\n",
    "\n",
    "No. of nodes at level 1 = 21 = 2\n",
    "\n",
    "...............................................................\n",
    "\n",
    "No. of nodes at level log10/9(n) = 2log10/9(n) = nlog10/9(2)\n",
    "\n",
    "Cost of sub problems at level log10/9(n) (last level) = nlog10/9(2) x T(1) = nlog10/9(2) x 1 = nlog10/9(2)  \n",
    "\n",
    "Step 4: Sum up the cost all the levels in recursive tree  \n",
    "T(n) = n + n + n + ---- + (no. of levels - 1) times + last level cost\n",
    "\n",
    " = n + n + n + ---- + log10/9(n) times + Θ(nlog10/9(2))\n",
    "\n",
    "= nlog10/9(n) + Θ(nlog10/9(2))\n",
    "\n",
    "Thus, T(n) = Θ(nlog10/9(2))\n",
    "\n",
    "Discussions\n",
    "( Threads )\n",
    "\n",
    "Most Recent\n",
    "User\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Commenting as Abbas Khan\n",
    "\n",
    "Comment Anonymously\n",
    "Submit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4392385",
   "metadata": {},
   "source": [
    "Space Complexity\n",
    "\n",
    "comments\n",
    "Space Complexity: \n",
    "The term Space Complexity is misused for Auxiliary Space at many places. Following are the correct definitions of Auxiliary Space and Space Complexity. \n",
    "\n",
    "Auxiliary Space is the extra space or temporary space used by an algorithm. \n",
    "\n",
    "Space Complexity of an algorithm is the total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. \n",
    "\n",
    "For example, if we want to compare standard sorting algorithms on the basis of space, then Auxiliary Space would be a better criterion than Space Complexity. Merge Sort uses O(n) auxiliary space, Insertion sort, and Heap Sort use O(1) auxiliary space. The space complexity of all these sorting algorithms is O(n) though.  \n",
    "\n",
    "Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two-dimensional array of size n*n, this will require O(n2) space.\n",
    "\n",
    "In recursive calls stack space also counts. \n",
    "\n",
    "Example : \n",
    "\n",
    "int add (int n){\n",
    "    if (n <= 0){\n",
    "        return 0;\n",
    "    }\n",
    "    return n + add (n-1);\n",
    "}\n",
    "\n",
    "Here each call add a level to the stack :\n",
    "\n",
    "1.  add(4)\n",
    "2.    -> add(3)\n",
    "3.      -> add(2)\n",
    "4.        -> add(1)\n",
    "5.          -> add(0)\n",
    "\n",
    "Each of these calls is added to call stack and takes up actual memory.\n",
    "So it takes O(n) space.\n",
    "However, just because you have n calls total doesn't mean it takes O(n) space.\n",
    "\n",
    "Look at the below function :\n",
    "\n",
    "int addSequence (int n){\n",
    "    int sum = 0;\n",
    "    for (int i = 0; i < n; i++){\n",
    "        sum += pairSum(i, i+1);\n",
    "    }\n",
    "    return sum;\n",
    "}\n",
    "\n",
    "int pairSum(int x, int y){\n",
    "    return x + y;\n",
    "}\n",
    "\n",
    "There will be roughly O(n) calls to pairSum. However, those \n",
    "calls do not exist simultaneously on the call stack,\n",
    "so you only need O(1) space.\n",
    "Note: It’s necessary to mention that space complexity depends on a variety of things such as the programming language, the compiler, or even the machine running the algorithm.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "Discussions\n",
    "( Threads )\n",
    "\n",
    "Most Recent\n",
    "Commenting as Abbas Khan\n",
    "\n",
    "Comment Anonymously\n",
    "Submit\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
